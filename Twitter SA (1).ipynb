{"cells":[{"cell_type":"code","source":["df =spark.sql(\"Select int(Sentiment) as label, tweet from training_twitter_shuff\")\ndf.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>1600000\n</div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["Importing important libraries\nThe libraries include:\n\npandas - data manipulation and wrangling\n\nnltk - natural language processessing\n\nsklearn - feature extraction\n\nbs4 - html handling\n\nre, string, itertools - Data manipulation\n\nlangid - Language detection."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\nfrom nltk import word_tokenize,WordNetLemmatizer,TweetTokenizer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tag import pos_tag\nimport nltk\nfrom bs4 import BeautifulSoup as bs\nimport re\nimport string\nfrom itertools import groupby\nimport langid\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["converting spark dataframe to pandas df for easy initial data exploration"],"metadata":{}},{"cell_type":"code","source":["pd_df=df.toPandas()\npd_df.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>\n   label                                              tweet\n0      4  It might be a good day to do the lotto. the we...\n1      0   Guys, that breaks my heart to hear you say that \n2      4              @JEWNNBUGG hey doll! How ya feeling? \n3      4  @redvers I&apos;ve been quite lucky this weekend......\n4      0                                   @longhorn_chyck \n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["pd_df['Len']=[len(i) for i in pd_df['tweet']]\npd_df.Len.plot(kind='box', by ='label')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["sample=pd_df[pd_df['Len']>140].sort_values('Len', ascending=False).tweet\nsample"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Python functions are created to clean the tweets to final modeling data.\n\nhtmlcleaning - converts html formatted tweets to actual tweets.\n\ntag_and_remove - keeps only nouns, adjectives, verbs and adverbs only in the tweets.\n\nlemmatize - converts words to its lemmatized form.\n\ncheck_lang - predict the language of the given text."],"metadata":{}},{"cell_type":"code","source":["def htmlcleaning(data_str):\n  return bs(data_str,\"lxml\").get_text()\n\n\ndef tag_and_remove(data_str):\n    nltk.download('averaged_perceptron_tagger')\n    cleaned_str = ' '\n    # noun tags\n    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n    # adjectives\n    jj_tags = ['JJ', 'JJR', 'JJS']\n    # verbs\n    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n    # adverbs\n    avb_tags =['RB','RBR','RBS']\n    \n    nltk_tags = nn_tags + jj_tags + vb_tags + avb_tags\n    # break string into 'words'\n    text = data_str.split()\n    # tag the text and keep only those with the right tags\n    tagged_text = pos_tag(text)\n    #print(tagged_text)\n    for tagged_word in tagged_text:\n        if tagged_word[1] in nltk_tags:\n            cleaned_str += tagged_word[0] + ' '\n    return cleaned_str\n  \n  \ndef lemmatize(data_str):\n    nltk.download('wordnet')\n    # expects a string\n    list_pos = 0\n    cleaned_str = ''\n    lmtzr = WordNetLemmatizer()\n    text = data_str.split()\n    tagged_words = pos_tag(text)\n    for word in tagged_words:\n        if 'v' in word[1].lower():\n            lemma = lmtzr.lemmatize(word[0], pos='v')\n        else:\n            lemma = lmtzr.lemmatize(word[0], pos='n')\n        if list_pos == 0:\n            cleaned_str = lemma\n        else:\n            cleaned_str = cleaned_str + ' ' + lemma\n        list_pos += 1\n    return cleaned_str\n  \n  \ndef check_lang(data_str):\n    predict_lang = langid.classify(data_str)\n    if predict_lang[1] >= .9:\n        language = predict_lang[0]\n    else:\n        language = 'NA'\n    return language"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["cleaning_irrelevant - cleaning irrelevant words from the tweets \n1. handing tweeting word (words with repeated characters usually used in tweets)\n2. handling apostrophes words\n3. removing urls and hyperlinks\n4. handling punctuations\n5. handling numbers and alphanumeric words\n6. remove @callouts and #mentions"],"metadata":{}},{"cell_type":"code","source":["def cleaning_irrelevant(data_str):\n  #handling tweeting word\n  commonBigrams=['l','s','e','o','t','f','p','r','m','c','n','d','g','i','b']\n  data_str=''.join(''.join(i)[:2] if value in commonBigrams else ''.join(i)[:1] for value,i in groupby(data_str))\n  #handling aphostrophe words\n  apostrophes={\"i'm\":\"i am\",\"aren't\":\"are not\",\"can't\":\"cannot\",\"couldn't\":\"could not\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\"don't\":\"do not\",\"hadn't\":\"had not\",\"hasn't\":\"has not\",\"haven't\":\"have not\",\"he'd\":\"he had\",\"he'll\":\"he will\",\"he's\":\"he is\",\"I'd\":\"I had\",\"I'll\":\"I will\",\"I'm\":\"I am\",\"I've\":\"I have\",\"isn't\":\"is not\",\"it's\":\"it is\",\"let's\":\"let us\",\"mustn't\":\"must not\",\"shan't\":\"shall not\",\"she'd\":\"she had\",\"she'll\":\"she will\",\"she's\":\"she is\",\"shouldn't\":\"should not\",\"that's\":\"that is\",\"there's\":\"there is\",\"they'd\":\"they had\",\"they'll\":\"they will\",\"they're\":\"they are\",\"they've\":\"they have\",\"we'd\":\"we had\",\"we're\":\"we are\",\"we've\":\"we have\",\"weren't\":\"were not\",\"what'll\":\"what will\",\"what're\":\"what are\",\"what's\":\"what is\",\"what've\":\"what have\",\"where's\":\"where is\",\"who'd\":\"who had\",\"who'll\":\"who will\",\"who're\":\"who are\",\"who's\":\"who is\",\"who've\":\"who have\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"you'd\":\"you had\",\"you'll\":\"you will\",\"you're\":\"you are\",\"you've\":\"you have\",\"y'll\":\"you all\"}\n  data_str=' '.join(apostrophes[word.lower()] if word.lower() in apostrophes else word for word in data_str.split())\n  #handling urls\n  url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n  #handling callouts & Hashtags\n  mention_re = re.compile('[@#](\\w+)')\n  #handling punctuations\n  punc_re = re.compile('[%s]' %re.escape(string.punctuation))\n  #handling numbers\n  num_re = re.compile('(\\\\d+)')\n  #handling valid words\n  alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n  #stop words\n  #stop_words=set(stopwords.words('english'))\n  # convert to lowercase\n  data_str = data_str.lower()\n  # remove hyperlinks\n  data_str = url_re.sub(' ', data_str)\n  # remove @mentions\n  data_str = mention_re.sub(' ', data_str)\n  # remove puncuation\n  data_str = punc_re.sub(' ', data_str)\n  # remove numeric 'words'\n  data_str = num_re.sub(' ', data_str)\n  data_str=' '.join(word for word in data_str.split() if alpha_num_re.match(word)) #and not word in stop_words)\n  return data_str"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["def tag_and_remove(data_str):\n    cleaned_str = ' '\n    # noun tags\n    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n    # adjectives\n    jj_tags = ['JJ', 'JJR', 'JJS']\n    # verbs\n    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n    # adverbs\n    avb_tags =['RB','RBR','RBS']\n    \n    nltk_tags = nn_tags + jj_tags + vb_tags+avb_tags\n    # break string into 'words'\n    text = data_str.split()\n    # tag the text and keep only those with the right tags\n    tagged_text = pos_tag(text)\n    #print(tagged_text)\n    for tagged_word in tagged_text:\n        if tagged_word[1] in nltk_tags:\n            cleaned_str += tagged_word[0] + ' '\n    return cleaned_str"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["def lemmatize(data_str):\n    # expects a string\n    list_pos = 0\n    cleaned_str = ''\n    lmtzr = WordNetLemmatizer()\n    text = data_str.split()\n    tagged_words = pos_tag(text)\n    for word in tagged_words:\n        if 'v' in word[1].lower():\n            lemma = lmtzr.lemmatize(word[0], pos='v')\n        else:\n            lemma = lmtzr.lemmatize(word[0], pos='n')\n        if list_pos == 0:\n            cleaned_str = lemma\n        else:\n            cleaned_str = cleaned_str + ' ' + lemma\n        list_pos += 1\n    return cleaned_str\ndef check_lang(data_str):\n    predict_lang = langid.classify(data_str)\n    if predict_lang[1] >= .9:\n        language = predict_lang[0]\n    else:\n        language = 'NA'\n    return language"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["for tweet in mini:\n  print(lemmatize(tag_and_remove(cleaning_irrelevant(htmlcleaning(tweet)))))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["converting python functions to pyspark udfs using udf and stringtype functions"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["remove_features_udf = udf(cleaning_irrelevant, StringType())\ntag_and_remove_udf = udf(tag_and_remove, StringType())\nlemmatize_udf = udf(lemmatize, StringType())\nhtmlcleaning_udf = udf(htmlcleaning, StringType())\ncheck_lang_udf=udf(check_lang,StringType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.functions import length\nclean_df=df.withColumn(\"clean\",lemmatize_udf(tag_and_remove_udf(remove_features_udf(htmlcleaning_udf((df['tweet']))))))\nclean_df_2 = clean_df.select('label','clean').filter(length(clean_df.clean)>0)\nclean_df_2.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>1594604\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Split the data into training and test sets (40% held out for testing)\n(trainingData, testData) = clean_df_2.randomSplit([0.9, 0.1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer,NGram,VectorAssembler,SQLTransformer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import NaiveBayes, RandomForestClassifier,LogisticRegression\nfrom pyspark.sql.functions import col, concat \nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\ntokenizer = Tokenizer(inputCol=\"clean\", outputCol=\"word\")\n\nngram = NGram(n=2, inputCol=\"word\", outputCol=\"ngrams\")\n\nsql = SQLTransformer(statement=\"SELECT *, concat(word,ngrams) AS word_comb FROM __THIS__\")\n\nhashingTF = HashingTF(inputCol=\"word_comb\", outputCol=\"rawFeatures\")\n\nidf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n\n# Logistic regression model\nlogistic = LogisticRegression(maxIter=10,regParam=0.001,elasticNetParam=1)\n\n# Pipeline Architecture\n\npipeline=Pipeline(stages=[\n    tokenizer, \n    ngram,\n    sql,\n    hashingTF,\n    idf,\n    logistic\n])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["paramGrid = ParamGridBuilder().addGrid(logistic.regParam, [0.0001,0.001,0.01,0.1,1,10]).addGrid(logistic.elasticNetParam,[0,0.1,0.9,1]).build()\ncv = CrossValidator(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid)\n\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["model = cvModel.bestModel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["predictions = model.transform(testData)\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3951970524638201&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>predictions <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>testData<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansigreen\">from</span> pyspark<span class=\"ansiyellow\">.</span>ml<span class=\"ansiyellow\">.</span>evaluation <span class=\"ansigreen\">import</span> BinaryClassificationEvaluator<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> evaluator <span class=\"ansiyellow\">=</span> BinaryClassificationEvaluator<span class=\"ansiyellow\">(</span>rawPredictionCol<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;prediction&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> evaluator<span class=\"ansiyellow\">.</span>evaluate<span class=\"ansiyellow\">(</span>predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;model&apos; is not defined</div>"]}}],"execution_count":24},{"cell_type":"code","source":["predictions = model.transform(testData)\n#predictions.show()\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["mylist=[\"I like this project\",\n        \"We love this course\",\n        \"i am not excited that the course is going to end\",\n        \"not bad\"]\ntest=spark.createDataFrame(mylist, StringType())\ntest=test.selectExpr(\"value as tweet\")\n\ntest_df=test.withColumn(\"clean\",lemmatize_udf(tag_and_remove_udf(remove_features_udf(htmlcleaning_udf(test['tweet'])))))\n\n#test_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["pred_test = model.transform(test_df)\nselected = pred_test.select(\"clean\", \"probability\", \"prediction\")\nfor row in selected.collect():\n    clean, prob, prediction = row\n    if prediction == 4:\n      pred = \"Positive\"\n    else:\n      pred = \"Negative\"\n    print(\"%s -->prediction=%s\" % (clean, pred))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">i project --&gt;prediction=Positive\nlove course --&gt;prediction=Positive\ni be not excited course be go end --&gt;prediction=Negative\nnot bad --&gt;prediction=Negative\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Exporting and importing model using mleap library"],"metadata":{}},{"cell_type":"code","source":["import mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\n\nserializedmodel= model.transform(trainingData)\n#display(serializedmodel)\n\nmodel.serializeToBundle(\"jar:file:/tmp/model-json.zip\", serializedmodel)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\ndeserializedPipeline = PipelineModel.deserializeFromBundle(\"jar:file:/FileStore/model-json.zip\")\ndisplay(deserializedPipeline)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"Twitter SA","notebookId":3951970524638178},"nbformat":4,"nbformat_minor":0}
